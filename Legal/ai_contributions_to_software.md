# Policy Guidelines for AI-Assisted Code Generation and AI Agents

## 1. Philosophy and Vision

At ECMWF, we view AI Code Agents and Large Language Models (LLMs) as powerful productivity tools and valuable collaborators in our software development process. The goal of these guidelines is **not** to slow down innovation or reduce the pace of AI adoption. Instead, they aim to ensure that as we increase our productivity, we maintain comprehensive knowledge, control, and security over our codebase.

## 2. Core Operational Principles

### Human-in-the-Loop and Accountability

* **Ultimate Responsibility:** There must always be a human-in-the-loop. The human developer who accepts, adapts, or merges AI-generated code assumes responsibility for its functionality, performance, and integration into the ECMWF codebase.
* **Understanding the Code:** Developers must understand the code they are committing. Do not treat AI-generated as a "black box."

### Branching and Merging Rules

* **No Direct Merges to Main:** Code Agents are strictly prohibited from merging code automatically into the `main` or operational branches.
* **Standard Contributor Treatment:** AI Agents must commit their generated code into separate feature branches. These branches are then subject to the exact same integration and review rules as code provided by any external human contributor.

### Code Review Protocols

* **Mandatory Review:** Code generated by AI Agents is always subject to review, as customary for any code contribution. This should include both automated checks and human peer review.
* **Peer Reviewing:** AI-generated code should be reviewed by a human *other* than the developer who prompted or controlled the AI agent. We recognise this may not always be possible, but it should be the preferred approach.
* **Operational Note:** This independent peer-review approach becomes even more important, preferred and highly recommended for any code entering operational, critical, or heavily relied-upon execution paths.

### Security and Asset Protection

* **Verification:** The responsible human developer must actively verify the AI-generated code for security vulnerabilities, logic flaws, or accidental damage to ECMWF assets.
* **Standard Precaution:** Treat AI-generated code with the same scrutiny for malicious or accidental flaws as you would any code submitted by an unknown external party.

### Managing External Contributions

* **Transparency:** When accepting pull requests from outside contributors, sub-contractors, or the open-source community, maintainers should seek to clarify if the code was generated by an AI Agent.
* **Reviewing External Merges:** As the internal ECMWF maintainer, you take full responsibility for the code once it is merged. This applies to any code, generated or not by AI Agents. Ensure external AI-assisted code meets our internal quality and security standards before integration.

### Working with AI Agents

* **Data Privacy and Prompt Security:** Developers must not paste sensitive ECMWF data, proprietary keys, unreleased model architectures, or sensitive infrastructure details into *public* AI prompts (e.g., consumer ChatGPT) versus approved internal enterprise tools like GitHub Copilot Enterprise. We have guarrantees from our enterprise providers that data kept secure, but we should still avoid sharing sensitive information in prompts.

* **Warning on Intellectual Property and Licensing:** Note that AI models can occasionally reproduce snippets of copyrighted code (e.g., strictly GPL-licensed code). Developers should ensure the generated code does not violate ECMWF's open-source licensing strategy (mostly Apache 2.0).

* **Test Coverage:** As for all external contributions, it is highly desirable that AI-generated code should be accompanied by unit tests. However, it is more important for AI generated code that *some* human-verified unit tests are included. AI is notoriously good at writing code that *looks* right but fails edge cases, making strict testing coverage even more important.
